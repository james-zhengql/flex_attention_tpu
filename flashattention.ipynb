{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyNJ7R5NHZbVvoZaAgauIYYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/james-zhengql/flex_attention_tpu/blob/main/flashattention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D1-nsVh_QiNR"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import functools\n",
        "from typing import Callable\n",
        "\n",
        "import jax\n",
        "from jax.experimental import pallas as pl\n",
        "from jax.experimental.pallas import tpu as pltpu\n",
        "from jax import random\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import time\n",
        "import statistics as stats\n",
        "\n",
        "\n",
        "dimension_numbers = (((1,), (1,)), ((), ()))\n",
        "MIN_BLOCK_SIZE = 128\n",
        "\n",
        "\n",
        "def _flash_attention_kernel(q_tile_ref, *args, **kwargs):\n",
        "    \"\"\"Connects _flash_attention_impl to the generated kernel.\"\"\"\n",
        "    block_b = q_tile_ref.shape[0]\n",
        "\n",
        "    # Create the real kernel from the factory\n",
        "    kernel = make_flash_attention_kernel()\n",
        "\n",
        "    for batch_idx in range(block_b):\n",
        "        kernel(\n",
        "            (batch_idx, 0),\n",
        "            q_tile_ref,\n",
        "            *args,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "\n",
        "def _flash_attention_impl(\n",
        "    q,\n",
        "    k,\n",
        "    v,\n",
        "    ab,\n",
        "    segment_ids,\n",
        "    save_residuals,\n",
        "    causal,\n",
        "    sm_scale,\n",
        "    block_b,\n",
        "    block_q,\n",
        "    block_k_major,\n",
        "    block_k,\n",
        "    debug,\n",
        "):\n",
        "  batch_size, num_heads, q_seq_len, head_dim = q.shape\n",
        "  _, _, kv_seq_len, _ = k.shape\n",
        "\n",
        "  # Grid specification\n",
        "  grid = (\n",
        "      pl.cdiv(batch_size, block_b),\n",
        "      num_heads,\n",
        "      pl.cdiv(q_seq_len, block_q),\n",
        "      kv_seq_len // block_k_major,\n",
        "  )\n",
        "\n",
        "  def q_index_map(batch_index, head_index, q_seq_index, _):\n",
        "    return (batch_index, head_index, q_seq_index, 0)\n",
        "\n",
        "  def kv_index_map(batch_index, head_index, q_seq_index, kv_seq_index):\n",
        "    next_kv_index = kv_seq_index\n",
        "    return (batch_index, head_index, next_kv_index, 0)\n",
        "\n",
        "  def ab_index_map(batch_index, head_index, q_seq_index, kv_seq_index):\n",
        "    next_q_index = q_seq_index\n",
        "    next_kv_index = kv_seq_index\n",
        "    return (batch_index, head_index, next_q_index, next_kv_index)\n",
        "\n",
        "  def o_index_map(batch_index, head_index, q_seq_index, _):\n",
        "    return (batch_index, head_index, q_seq_index, 0)\n",
        "\n",
        "  def lm_index_map(batch_index, head_index, q_seq_index, _):\n",
        "    return (batch_index, head_index, q_seq_index, 0)\n",
        "\n",
        "  kernel = functools.partial(\n",
        "      _flash_attention_kernel,\n",
        "      causal=causal,\n",
        "      sm_scale=sm_scale,\n",
        "      block_k=block_k,\n",
        "      kv_seq_len=kv_seq_len,\n",
        "  )\n",
        "\n",
        "  out_shape = jax.ShapeDtypeStruct(shape=q.shape, dtype=q.dtype)\n",
        "  out_shape = [out_shape]\n",
        "  out_specs = [pl.BlockSpec((block_b, 1, block_q, head_dim), o_index_map)]\n",
        "\n",
        "  # Allocate scratch buffers\n",
        "  if block_k != kv_seq_len:\n",
        "    m_scratch = pltpu.VMEM((block_b, 1, block_q, MIN_BLOCK_SIZE), jnp.float32)\n",
        "    l_scratch = pltpu.VMEM((block_b, 1, block_q, MIN_BLOCK_SIZE), jnp.float32)\n",
        "    acc_scratch = pltpu.VMEM((block_b, 1, block_q, head_dim), jnp.float32)\n",
        "    scratch_shapes = [m_scratch, l_scratch, acc_scratch]\n",
        "  else:\n",
        "    scratch_shapes = []\n",
        "\n",
        "  # Output specs\n",
        "  if save_residuals:\n",
        "    out_specs = [\n",
        "        *out_specs,\n",
        "        pl.BlockSpec((block_b, 1, block_q, MIN_BLOCK_SIZE), lm_index_map),\n",
        "        pl.BlockSpec((block_b, 1, block_q, MIN_BLOCK_SIZE), lm_index_map),\n",
        "    ]\n",
        "    l = jax.ShapeDtypeStruct(\n",
        "        (batch_size, num_heads, q_seq_len, MIN_BLOCK_SIZE), dtype=jnp.float32\n",
        "    )\n",
        "    m = jax.ShapeDtypeStruct(\n",
        "        (batch_size, num_heads, q_seq_len, MIN_BLOCK_SIZE), dtype=jnp.float32\n",
        "    )\n",
        "    out_shape = (*out_shape, l, m)\n",
        "  else:\n",
        "    out_specs = [*out_specs, None, None]\n",
        "    out_shape = (*out_shape, None, None)\n",
        "\n",
        "  ab_block_spec = (\n",
        "      pl.BlockSpec((block_b, 1, block_q, block_k_major), ab_index_map)\n",
        "      if ab is not None else None)\n",
        "\n",
        "  in_specs = [\n",
        "      pl.BlockSpec((block_b, 1, block_q, head_dim), q_index_map),\n",
        "      pl.BlockSpec((block_b, 1, block_k_major, head_dim), kv_index_map),\n",
        "      pl.BlockSpec((block_b, 1, block_k_major, head_dim), kv_index_map),\n",
        "      ab_block_spec,\n",
        "  ]\n",
        "\n",
        "  o, *aux = pl.pallas_call(\n",
        "      kernel,\n",
        "      grid_spec=pltpu.PrefetchScalarGridSpec(\n",
        "          num_scalar_prefetch=0,\n",
        "          grid=grid,\n",
        "          in_specs=in_specs,\n",
        "          out_specs=out_specs,\n",
        "          scratch_shapes=scratch_shapes,\n",
        "      ),\n",
        "      out_shape=out_shape,\n",
        "      debug=debug,\n",
        "      compiler_params=pltpu.CompilerParams(\n",
        "          dimension_semantics=(\"parallel\", \"parallel\", \"parallel\", \"arbitrary\")\n",
        "      ),\n",
        "  )(q, k, v, ab)\n",
        "\n",
        "  if save_residuals:\n",
        "    l, m = (v[..., 0] for v in aux[-2:])\n",
        "    return (o, l, m)\n",
        "  else:\n",
        "    return o\n",
        "\n",
        "\n",
        "def mha_reference(\n",
        "    q,\n",
        "    k,\n",
        "    v,\n",
        "    ab: jax.Array | None = None,\n",
        "    *,\n",
        "    sm_scale: float = 1.0,\n",
        "    save_residuals: bool = False,\n",
        "):\n",
        "  logits = jnp.einsum(\"bhqc,bhkc->bhqk\", q, k)\n",
        "  if ab is not None:\n",
        "    logits += ab\n",
        "  if sm_scale != 1.0:\n",
        "    logits *= sm_scale\n",
        "\n",
        "  # --- causal masking (disabled for now but can enable later)\n",
        "  mask = None\n",
        "  # if causal:\n",
        "  #   _, _, q_seq_len, _ = q.shape\n",
        "  #   _, _, kv_seq_len, _ = k.shape\n",
        "  #   mask_shape = (q_seq_len, kv_seq_len)\n",
        "  #   row_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0)\n",
        "  #   col_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)\n",
        "  #   causal_mask = (col_ids <= row_ids)[None, None, :, :]\n",
        "  #   mask = causal_mask if mask is None else jnp.logical_and(mask, causal_mask)\n",
        "\n",
        "  logits = logits if mask is None else logits + jnp.where(mask, 0.0, -1e9)\n",
        "\n",
        "  m = logits.max(axis=-1)\n",
        "  unnormalized = jnp.exp(logits - m[..., None])\n",
        "  l = unnormalized.sum(axis=-1)\n",
        "  weights = unnormalized / l[..., None]\n",
        "  out = jnp.einsum(\"bhqk,bhkc->bhqc\", weights, v)\n",
        "  if save_residuals:\n",
        "    return out, l, m\n",
        "  return out\n",
        "\n",
        "\n",
        "def make_flash_attention_kernel(mask_fn=None):\n",
        "  \"\"\"Factory returning a kernel with an optional custom mask function.\"\"\"\n",
        "  def flash_attention_fwd_kernel(\n",
        "      batch_idx,\n",
        "      q_tile_ref,\n",
        "      k_tile_ref,\n",
        "      v_tile_ref,\n",
        "      ab_tile_ref,\n",
        "      O_tile_ref,\n",
        "      m_tile_ref,\n",
        "      l_tile_ref,\n",
        "      O_scratch_ref,\n",
        "      m_scratch_ref,\n",
        "      l_scratch_ref,\n",
        "      *,\n",
        "      causal,\n",
        "      sm_scale,\n",
        "      block_k,\n",
        "      kv_seq_len,\n",
        "  ):\n",
        "    block_k_major = k_tile_ref.shape[2]\n",
        "    head_dim = k_tile_ref.shape[3]\n",
        "    kv_seq_idx = pl.program_id(3)\n",
        "\n",
        "    @pl.when(kv_seq_idx == 0)\n",
        "    def start_new_seq():\n",
        "      m_scratch_ref[batch_idx] = jnp.full(\n",
        "          m_scratch_ref.shape[2:], -jnp.inf, jnp.float32)\n",
        "      l_scratch_ref[batch_idx] = jnp.zeros(\n",
        "          l_scratch_ref.shape[2:], jnp.float32)\n",
        "      O_scratch_ref[batch_idx] = jnp.zeros(\n",
        "          O_scratch_ref.shape[2:], jnp.float32)\n",
        "\n",
        "    if mask_fn is None:\n",
        "      should_run = True\n",
        "\n",
        "    @pl.when(should_run)\n",
        "    def body():\n",
        "      @pl.loop(0, block_k_major, step=block_k, unroll=True)\n",
        "      def _body(start_k):\n",
        "        m_past = m_scratch_ref[batch_idx]\n",
        "        l_past = l_scratch_ref[batch_idx]\n",
        "        O_past = O_scratch_ref[batch_idx]\n",
        "        k_ref = k_tile_ref[(*batch_idx, pl.dslice(start_k, block_k), slice(None))]\n",
        "        q_ref = q_tile_ref[batch_idx]\n",
        "\n",
        "        S = jax.lax.dot_general(q_ref, k_ref, dimension_numbers, preferred_element_type=jnp.float32)\n",
        "        S *= sm_scale\n",
        "\n",
        "        if ab_tile_ref is not None:\n",
        "          ab = ab_tile_ref[\n",
        "              (*batch_idx, pl.dslice(None), pl.dslice(start_k, block_k))\n",
        "          ].astype(jnp.float32)\n",
        "          S += ab\n",
        "\n",
        "        m_cur = jnp.max(S, axis=1)[:, None]\n",
        "        m_next = jnp.maximum(m_cur, m_past)\n",
        "        block_k_repeats, rem = divmod(block_k, MIN_BLOCK_SIZE)\n",
        "        if rem:\n",
        "          raise NotImplementedError(f\"{block_k=} should be a multiple of {MIN_BLOCK_SIZE}\")\n",
        "\n",
        "        P = jnp.exp(S - pltpu.repeat(m_next, block_k_repeats, 1))\n",
        "        l_corr = jnp.exp(m_past - m_next) * l_past\n",
        "        l_next = l_corr + jnp.sum(P, axis=1)[:, None]\n",
        "\n",
        "        head_dim_repeats, rem = divmod(head_dim, MIN_BLOCK_SIZE)\n",
        "        if rem:\n",
        "          raise NotImplementedError(f\"{head_dim=} should be a multiple of {MIN_BLOCK_SIZE} if larger\")\n",
        "\n",
        "        l_broadcast = lambda l: pltpu.repeat(l, head_dim_repeats, 1)\n",
        "        l_scratch_ref[batch_idx] = l_next\n",
        "        m_scratch_ref[batch_idx] = m_next\n",
        "\n",
        "        l_next_inv_safe = jnp.where(l_next == 0.0, 1.0, 1.0 / l_next)\n",
        "        v_ref = v_tile_ref[(*batch_idx, pl.dslice(start_k, block_k), slice(None))]\n",
        "        o_curr = jax.lax.dot(P.astype(v_ref.dtype), v_ref, preferred_element_type=jnp.float32)\n",
        "        O_scratch_ref[batch_idx] = O_past * l_broadcast(l_corr) + o_curr\n",
        "        O_scratch_ref[batch_idx] *= l_broadcast(l_next_inv_safe)\n",
        "\n",
        "    @pl.when(kv_seq_idx == (kv_seq_len // block_k_major) - 1)\n",
        "    def store_res():\n",
        "      O_tile_ref[batch_idx] = O_scratch_ref[batch_idx].astype(O_tile_ref.dtype)\n",
        "      # Only store m/l if they were requested (i.e., not None)\n",
        "      if (m_tile_ref is not None) and (l_tile_ref is not None):\n",
        "        m_tile_ref[batch_idx] = m_scratch_ref[batch_idx].astype(m_tile_ref.dtype)\n",
        "        l_tile_ref[batch_idx] = l_scratch_ref[batch_idx].astype(l_tile_ref.dtype)\n",
        "\n",
        "  return flash_attention_fwd_kernel\n",
        "\n",
        "def flop_count_attention(b, h, q, k, d):\n",
        "    \"\"\"\n",
        "    Rough FLOP count for one forward pass of scaled dot-product attention:\n",
        "      QK^T  : 2 * b * h * q * k * d        (matrix multiplication)\n",
        "      Softmax: ~ b * h * q * k             (small, we ignore it)\n",
        "      (softmax @ V): 2 * b * h * q * k * d (another matmul)\n",
        "    Total ≈ 4 * b * h * q * k * d FLOPs\n",
        "    \"\"\"\n",
        "    return 4.0 * b * h * q * k * d\n",
        "\n",
        "def benchmark(fn, args, iters=30, warmup=5, name=\"fn\"):\n",
        "    # 1. Warmup phase — triggers JIT compilation and stabilizes cache\n",
        "    for _ in range(warmup):\n",
        "        y = fn(*args)\n",
        "        # .block_until_ready() ensures we wait until computation is finished\n",
        "        if isinstance(y, (tuple, list)):\n",
        "            jax.tree_util.tree_map(\n",
        "                lambda x: x.block_until_ready() if hasattr(x, \"block_until_ready\") else x, y\n",
        "            )\n",
        "        else:\n",
        "            y.block_until_ready()\n",
        "\n",
        "    # 2. Timed runs\n",
        "    times = []\n",
        "    for _ in range(iters):\n",
        "        t0 = time.perf_counter()\n",
        "        y = fn(*args)\n",
        "        # Synchronize (very important for accurate timing)\n",
        "        if isinstance(y, (tuple, list)):\n",
        "            jax.tree_util.tree_map(\n",
        "                lambda x: x.block_until_ready() if hasattr(x, \"block_until_ready\") else x, y\n",
        "            )\n",
        "        else:\n",
        "            y.block_until_ready()\n",
        "        t1 = time.perf_counter()\n",
        "        times.append(t1 - t0)\n",
        "\n",
        "    # 3. Compute summary statistics\n",
        "    mean_t = sum(times) / len(times)\n",
        "    med_t = stats.median(times)\n",
        "    p10, p90 = np.percentile(np.array(times), [10, 90])\n",
        "\n",
        "    print(f\"[{name}] mean={mean_t*1e3:.2f} ms  median={med_t*1e3:.2f} ms  \"\n",
        "          f\"p10={p10*1e3:.2f} ms  p90={p90*1e3:.2f} ms\")\n",
        "\n",
        "    # Return average and median latency (seconds)\n",
        "    return mean_t, med_t\n",
        "\n",
        "def build_fns_for_bench(\n",
        "    q, k, v,\n",
        "    *,\n",
        "    ab=None,\n",
        "    sm_scale=1.0,\n",
        "    save_residuals=False,\n",
        "    causal=False,\n",
        "    block_b=1,\n",
        "    block_q=128,\n",
        "    block_k_major=128,\n",
        "    block_k=128,\n",
        "    debug=False,\n",
        "):\n",
        "    # JIT-compile reference implementation\n",
        "    ref_fn = functools.partial(mha_reference, ab=None, sm_scale=sm_scale, save_residuals=False)\n",
        "    ref_jit = jax.jit(ref_fn)\n",
        "\n",
        "    # JIT-compile your custom Pallas kernel implementation\n",
        "    flash_partial = functools.partial(\n",
        "        _flash_attention_impl,\n",
        "        ab=ab,\n",
        "        segment_ids=None,\n",
        "        save_residuals=save_residuals,\n",
        "        causal=causal,\n",
        "        sm_scale=sm_scale,\n",
        "        block_b=block_b,\n",
        "        block_q=block_q,\n",
        "        block_k_major=block_k_major,\n",
        "        block_k=block_k,\n",
        "        debug=debug,\n",
        "    )\n",
        "    flash_jit = jax.jit(flash_partial)\n",
        "\n",
        "    return ref_jit, flash_jit\n",
        "\n",
        "def run_bench_suite(q, k, v, *, sm_scale, block_b, block_q, block_k_major, block_k, causal=False):\n",
        "    # Unpack shapes\n",
        "    b, h, q_len, d = q.shape\n",
        "    _, _, k_len, _ = k.shape\n",
        "\n",
        "    # Compute FLOPs (for throughput calculation)\n",
        "    gflops = flop_count_attention(b, h, q_len, k_len, d) / 1e9\n",
        "\n",
        "    # Create JIT-compiled versions of both implementations\n",
        "    ref_jit, flash_jit = build_fns_for_bench(\n",
        "        q, k, v,\n",
        "        sm_scale=sm_scale,\n",
        "        save_residuals=False,  # exclude residual buffers for fair comparison\n",
        "        causal=causal,\n",
        "        block_b=block_b,\n",
        "        block_q=block_q,\n",
        "        block_k_major=block_k_major,\n",
        "        block_k=block_k,\n",
        "        debug=False,\n",
        "    )\n",
        "\n",
        "    print(f\"\\n== Benchmark config: b={b}, h={h}, q={q_len}, k={k_len}, d={d}, causal={causal} ==\")\n",
        "    print(f\"Estimated FLOPs per call: {gflops:.2f} GFLOPs\")\n",
        "\n",
        "    # ---- Reference (naive) implementation ----\n",
        "    t_mean_ref, t_med_ref = benchmark(ref_jit, (q, k, v), name=\"mha_reference[jit]\")\n",
        "    print(f\"  → Throughput: {gflops / t_med_ref:.2f} GFLOP/s\")\n",
        "\n",
        "    # ---- Pallas FlashAttention kernel ----\n",
        "    t_mean_flash, t_med_flash = benchmark(flash_jit, (q, k, v), name=\"pallas_flash[jit]\")\n",
        "    print(f\"  → Throughput: {gflops / t_med_flash:.2f} GFLOP/s\")\n",
        "\n",
        "    # ---- Numeric correctness check ----\n",
        "    o_ref = ref_jit(q, k, v).block_until_ready()\n",
        "    o_flash = flash_jit(q, k, v)\n",
        "    if isinstance(o_flash, (tuple, list)):\n",
        "        o_flash = o_flash[0]\n",
        "    o_flash = o_flash.block_until_ready()\n",
        "\n",
        "    # Relative L2 error measures numerical difference between both results\n",
        "    rel_err = jnp.linalg.norm(o_flash - o_ref) / jnp.linalg.norm(o_ref)\n",
        "    print(f\"Numeric diff (Relative L2): {rel_err:.3e}\")\n",
        "\n",
        "    # Return summarized metrics\n",
        "    return {\n",
        "        \"ref_ms_med\": t_med_ref * 1e3,\n",
        "        \"flash_ms_med\": t_med_flash * 1e3,\n",
        "        \"ref_gflops\": gflops / t_med_ref,\n",
        "        \"flash_gflops\": gflops / t_med_flash,\n",
        "        \"rel_l2\": float(rel_err),\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _flash_attention_kernel_ref(q_tile_ref, *args, **kwargs):\n",
        "  block_b = q_tile_ref.shape[0]\n",
        "  # If we're not going to tile the softmax, then we can avoid a bunch of VPU ops.\n",
        "  kernel = _flash_attention_kernel_single_batch\n",
        "  for batch_idx in range(block_b):\n",
        "    kernel((batch_idx, 0), q_tile_ref, *args, **kwargs)\n",
        "\n",
        "\n",
        "def _flash_attention_kernel_single_batch(\n",
        "    batch_idx,\n",
        "    q_tile_ref,\n",
        "    k_tile_ref,\n",
        "    v_tile_ref,\n",
        "    ab_tile_ref,\n",
        "    o_tile_ref,  # Output arrays\n",
        "    l_ref,\n",
        "    m_ref,\n",
        "    m_scratch_ref,\n",
        "    l_scratch_ref,\n",
        "    acc_scratch_ref,\n",
        "    *,\n",
        "    causal,\n",
        "    sm_scale,\n",
        "    block_k,\n",
        "    kv_seq_len,\n",
        "):\n",
        "  block_k_major = k_tile_ref.shape[2]\n",
        "  block_q = q_tile_ref.shape[2]\n",
        "  head_dim = q_tile_ref.shape[-1]\n",
        "\n",
        "  kv_seq_idx = pl.program_id(3)\n",
        "  @pl.when(kv_seq_idx == 0)\n",
        "  def start_new_sequence():\n",
        "    m_scratch_ref[batch_idx] = jnp.full(\n",
        "        m_scratch_ref.shape[2:], -jnp.inf, jnp.float32\n",
        "    )\n",
        "    l_scratch_ref[batch_idx] = jnp.zeros(l_scratch_ref.shape[2:], jnp.float32)\n",
        "    acc_scratch_ref[batch_idx] = jnp.zeros(\n",
        "        acc_scratch_ref.shape[2:], jnp.float32\n",
        "    )\n",
        "\n",
        "  q_seq_idx = pl.program_id(2)\n",
        "\n",
        "  should_run = True\n",
        "\n",
        "  @pl.when(should_run)\n",
        "  def run():\n",
        "    @pl.loop(0, block_k_major, step=block_k, unroll=True)\n",
        "    def _body(start_k):\n",
        "      m_prev = m_scratch_ref[batch_idx]\n",
        "      l_prev = l_scratch_ref[batch_idx]\n",
        "      q = q_tile_ref[batch_idx]  # [block_q, head_dim]\n",
        "      k = k_tile_ref[\n",
        "          (*batch_idx, pl.dslice(start_k, block_k), slice(None))\n",
        "      ]  # [block_k, head_dim]\n",
        "\n",
        "      s = jax.lax.dot_general(\n",
        "          q, k, dimension_numbers, preferred_element_type=jnp.float32\n",
        "      )  # [block_q, block_k]\n",
        "\n",
        "      # Add attention bias if needed.\n",
        "      # TODO(tanburn) Should the attention bias be added before or after\n",
        "      # multiplication by sm_scale?\n",
        "      if ab_tile_ref is not None:\n",
        "        ab = ab_tile_ref[\n",
        "            (*batch_idx, pl.dslice(None), pl.dslice(start_k, block_k))\n",
        "        ].astype(jnp.float32)\n",
        "        s += ab\n",
        "\n",
        "      if sm_scale != 1.0:\n",
        "        s *= sm_scale\n",
        "\n",
        "      mask = None\n",
        "\n",
        "\n",
        "      m_curr = jnp.max(s, axis=1)[:, None]  # Row max, shape [block_q, 1].\n",
        "      m_next = jnp.maximum(m_prev, m_curr)  # Shape [block_q, 128].\n",
        "\n",
        "      block_k_repeats, rem = divmod(block_k, MIN_BLOCK_SIZE)\n",
        "      if rem:\n",
        "        raise NotImplementedError(\n",
        "            f\"{block_k=} should be a multiple of {MIN_BLOCK_SIZE}\"\n",
        "        )\n",
        "      p = jnp.exp(s - pltpu.repeat(m_next, block_k_repeats, 1))\n",
        "\n",
        "      alpha = jnp.exp(m_prev - m_next)  # Shape [block_q, 128].\n",
        "\n",
        "      l_corr = alpha * l_prev\n",
        "\n",
        "      l_next = jnp.sum(p, axis=1)[:, None] + l_corr  # Shape [block_q, 128]\n",
        "\n",
        "      head_dim_repeats, rem = divmod(head_dim, MIN_BLOCK_SIZE)\n",
        "      l_broadcast = lambda l: pltpu.repeat(l, head_dim_repeats, 1)\n",
        "      if rem:\n",
        "        if head_dim_repeats == 0:\n",
        "          l_broadcast = lambda l: l[:, :head_dim]\n",
        "        else:\n",
        "          raise NotImplementedError(\n",
        "              f\"{head_dim=} should be a multiple of {MIN_BLOCK_SIZE} if larger\"\n",
        "          )\n",
        "      l_scratch_ref[batch_idx] = l_next\n",
        "      m_scratch_ref[batch_idx] = m_next\n",
        "\n",
        "      l_next_inv_safe = jnp.where(l_next == 0.0, 1.0, 1.0 / l_next)\n",
        "      acc_scratch_ref[batch_idx] *= l_broadcast(l_corr * l_next_inv_safe)\n",
        "      v = v_tile_ref[(*batch_idx, pl.dslice(start_k, block_k), slice(None))]\n",
        "      o_curr = jax.lax.dot(\n",
        "          p.astype(v.dtype), v, preferred_element_type=jnp.float32\n",
        "      )\n",
        "      acc_scratch_ref[batch_idx] += o_curr * l_broadcast(l_next_inv_safe)\n",
        "\n",
        "  @pl.when(kv_seq_idx == (kv_seq_len // block_k_major) - 1)\n",
        "  def store_output():\n",
        "    o_tile_ref[batch_idx] = acc_scratch_ref[batch_idx].astype(o_tile_ref.dtype)\n",
        "    if l_ref is not None:\n",
        "      l_ref[batch_idx] = l_scratch_ref[batch_idx].astype(l_ref.dtype)\n",
        "    if m_ref is not None:\n",
        "      m_ref[batch_idx] = m_scratch_ref[batch_idx].astype(m_ref.dtype)\n",
        "\n",
        "def _flash_attention_impl_ref(\n",
        "    q,\n",
        "    k,\n",
        "    v,\n",
        "    ab,\n",
        "    segment_ids,\n",
        "    save_residuals,\n",
        "    causal,\n",
        "    sm_scale,\n",
        "    block_b,\n",
        "    block_q,\n",
        "    block_k_major,\n",
        "    block_k,\n",
        "    debug,\n",
        "):\n",
        "  batch_size, num_heads, q_seq_len, head_dim = q.shape\n",
        "  _, _, kv_seq_len, _ = k.shape\n",
        "\n",
        "  # Grid specification\n",
        "  grid = (\n",
        "      pl.cdiv(batch_size, block_b),\n",
        "      num_heads,\n",
        "      pl.cdiv(q_seq_len, block_q),\n",
        "      kv_seq_len // block_k_major,\n",
        "  )\n",
        "\n",
        "  def q_index_map(batch_index, head_index, q_seq_index, _):\n",
        "    return (batch_index, head_index, q_seq_index, 0)\n",
        "\n",
        "  def kv_index_map(batch_index, head_index, q_seq_index, kv_seq_index):\n",
        "    next_kv_index = kv_seq_index\n",
        "    return (batch_index, head_index, next_kv_index, 0)\n",
        "\n",
        "  def ab_index_map(batch_index, head_index, q_seq_index, kv_seq_index):\n",
        "    next_q_index = q_seq_index\n",
        "    next_kv_index = kv_seq_index\n",
        "    return (batch_index, head_index, next_q_index, next_kv_index)\n",
        "\n",
        "  def o_index_map(batch_index, head_index, q_seq_index, _):\n",
        "    return (batch_index, head_index, q_seq_index, 0)\n",
        "\n",
        "  def lm_index_map(batch_index, head_index, q_seq_index, _):\n",
        "    return (batch_index, head_index, q_seq_index, 0)\n",
        "\n",
        "  kernel = functools.partial(\n",
        "      _flash_attention_kernel_ref,\n",
        "      causal=causal,\n",
        "      sm_scale=sm_scale,\n",
        "      block_k=block_k,\n",
        "      kv_seq_len=kv_seq_len,\n",
        "  )\n",
        "\n",
        "  out_shape = jax.ShapeDtypeStruct(shape=q.shape, dtype=q.dtype)\n",
        "  out_shape = [out_shape]\n",
        "  out_specs = [pl.BlockSpec((block_b, 1, block_q, head_dim), o_index_map)]\n",
        "\n",
        "  # Allocate scratch buffers\n",
        "  if block_k != kv_seq_len:\n",
        "    m_scratch = pltpu.VMEM((block_b, 1, block_q, MIN_BLOCK_SIZE), jnp.float32)\n",
        "    l_scratch = pltpu.VMEM((block_b, 1, block_q, MIN_BLOCK_SIZE), jnp.float32)\n",
        "    acc_scratch = pltpu.VMEM((block_b, 1, block_q, head_dim), jnp.float32)\n",
        "    scratch_shapes = [m_scratch, l_scratch, acc_scratch]\n",
        "  else:\n",
        "    scratch_shapes = []\n",
        "\n",
        "  # Output specs\n",
        "  if save_residuals:\n",
        "    out_specs = [\n",
        "        *out_specs,\n",
        "        pl.BlockSpec((block_b, 1, block_q, MIN_BLOCK_SIZE), lm_index_map),\n",
        "        pl.BlockSpec((block_b, 1, block_q, MIN_BLOCK_SIZE), lm_index_map),\n",
        "    ]\n",
        "    l = jax.ShapeDtypeStruct(\n",
        "        (batch_size, num_heads, q_seq_len, MIN_BLOCK_SIZE), dtype=jnp.float32\n",
        "    )\n",
        "    m = jax.ShapeDtypeStruct(\n",
        "        (batch_size, num_heads, q_seq_len, MIN_BLOCK_SIZE), dtype=jnp.float32\n",
        "    )\n",
        "    out_shape = (*out_shape, l, m)\n",
        "  else:\n",
        "    out_specs = [*out_specs, None, None]\n",
        "    out_shape = (*out_shape, None, None)\n",
        "\n",
        "  ab_block_spec = (\n",
        "      pl.BlockSpec((block_b, 1, block_q, block_k_major), ab_index_map)\n",
        "      if ab is not None else None)\n",
        "\n",
        "  in_specs = [\n",
        "      pl.BlockSpec((block_b, 1, block_q, head_dim), q_index_map),\n",
        "      pl.BlockSpec((block_b, 1, block_k_major, head_dim), kv_index_map),\n",
        "      pl.BlockSpec((block_b, 1, block_k_major, head_dim), kv_index_map),\n",
        "      ab_block_spec,\n",
        "  ]\n",
        "\n",
        "  o, *aux = pl.pallas_call(\n",
        "      kernel,\n",
        "      grid_spec=pltpu.PrefetchScalarGridSpec(\n",
        "          num_scalar_prefetch=0,\n",
        "          grid=grid,\n",
        "          in_specs=in_specs,\n",
        "          out_specs=out_specs,\n",
        "          scratch_shapes=scratch_shapes,\n",
        "      ),\n",
        "      out_shape=out_shape,\n",
        "      debug=debug,\n",
        "      compiler_params=pltpu.CompilerParams(\n",
        "          dimension_semantics=(\"parallel\", \"parallel\", \"parallel\", \"arbitrary\")\n",
        "      ),\n",
        "  )(q, k, v, ab)\n",
        "\n",
        "  if save_residuals:\n",
        "    l, m = (v[..., 0] for v in aux[-2:])\n",
        "    return (o, l, m)\n",
        "  else:\n",
        "    return o\n"
      ],
      "metadata": {
        "id": "YCZIjFhSR0LH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# assume jax, jax.random, jnp, and your helper functions are already imported:\n",
        "# from jax import random\n",
        "# import jax.numpy as jnp\n",
        "# from your_module import mha_reference, _flash_attention_impl, _flash_attention_impl_ref\n",
        "\n",
        "def main():\n",
        "    key = random.PRNGKey(0)\n",
        "    batch = 1\n",
        "    heads = 1\n",
        "    q_len = 25600\n",
        "    kv_len = 25600\n",
        "    head_dim = 128\n",
        "\n",
        "    k1, k2, k3 = random.split(key, 3)\n",
        "    q = random.normal(k1, (batch, heads, q_len, head_dim), dtype=jnp.float32)\n",
        "    k = random.normal(k2, (batch, heads, kv_len, head_dim), dtype=jnp.float32)\n",
        "    v = random.normal(k3, (batch, heads, kv_len, head_dim), dtype=jnp.float32)\n",
        "    ab = None\n",
        "    segment_ids = None\n",
        "\n",
        "    block_b = 1\n",
        "    block_q = 512\n",
        "    block_k_major = 512\n",
        "    block_k = 128\n",
        "\n",
        "    causal = False\n",
        "    # stable scalar softmax scale\n",
        "    sm_scale = 1.0 / math.sqrt(float(head_dim))\n",
        "    debug = False\n",
        "    save_residuals = True\n",
        "\n",
        "    print(\"Running reference attention (for numeric check)...\")\n",
        "    ref = mha_reference(q, k, v, sm_scale=sm_scale)\n",
        "\n",
        "    print(\"Running Pallas TPU flash attention kernel...\")\n",
        "    out = _flash_attention_impl(\n",
        "        q=q, k=k, v=v, ab=ab, segment_ids=segment_ids,\n",
        "        save_residuals=save_residuals,\n",
        "        causal=causal, sm_scale=sm_scale,\n",
        "        block_b=block_b, block_q=block_q,\n",
        "        block_k_major=block_k_major, block_k=block_k,\n",
        "        debug=debug,\n",
        "    )\n",
        "\n",
        "    print(\"Running Pallas TPU ref flash attention kernel...\")\n",
        "    out_ref = _flash_attention_impl_ref(\n",
        "        q=q, k=k, v=v, ab=ab, segment_ids=segment_ids,\n",
        "        save_residuals=save_residuals,\n",
        "        causal=causal, sm_scale=sm_scale,\n",
        "        block_b=block_b, block_q=block_q,\n",
        "        block_k_major=block_k_major, block_k=block_k,\n",
        "        debug=debug,\n",
        "    )\n",
        "\n",
        "    # unpack outputs correctly whether residuals were saved or not\n",
        "    if save_residuals:\n",
        "        o, l, m = out\n",
        "        out_ref_o, out_ref_l, out_ref_m = out_ref\n",
        "    else:\n",
        "        o = out\n",
        "        out_ref_o = out_ref\n",
        "\n",
        "    # correctness check (compare output tensors)\n",
        "    diff = jnp.linalg.norm(o - out_ref_o) / jnp.linalg.norm(ref)\n",
        "    print(f\"Relative L2 error vs reference: {diff:.3e}\")\n",
        "    print(\"Output shape:\", o.shape)\n",
        "\n",
        "    # performance comparison (disabled)\n",
        "    # results = run_bench_suite(...)\n",
        "    # print(\"\\nSummary:\", results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xgcz1LhTKxO",
        "outputId": "16ccb072-7276-4558-e6a7-54a3f6ee3503"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running reference attention (for numeric check)...\n",
            "Running Pallas TPU flash attention kernel...\n",
            "Running Pallas TPU ref flash attention kernel...\n",
            "Relative L2 error vs reference: 5.910e-07\n",
            "Output shape: (1, 1, 25600, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U jax jaxlib libtpu-nightly -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -U \"jax[tpu]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHh1U4OuRFvb",
        "outputId": "988f0e1f-96fc-46e9-8637-8fcaa6e7986c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Collecting jax\n",
            "  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 0.0.1\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement libtpu-nightly (from versions: 0.1.dev20210615, 0.1.dev20210709, 0.1.dev20210809, 0.1.dev20210916, 0.1.dev20210917, 0.1.dev20210920, 0.1.dev20210921, 0.1.dev20210922, 0.1.dev20210923, 0.1.dev20210924, 0.1.dev20210925, 0.1.dev20210926, 0.1.dev20210927, 0.1.dev20210928, 0.1.dev20210929, 0.1.dev20210930, 0.1.dev20211001, 0.1.dev20211002, 0.1.dev20211003, 0.1.dev20211004, 0.1.dev20211005, 0.1.dev20211006, 0.1.dev20211007+default, 0.1.dev20211008, 0.1.dev20211009, 0.1.dev20211010, 0.1.dev20211011, 0.1.dev20211012, 0.1.dev20211013, 0.1.dev20211014, 0.1.dev20211015, 0.1.dev20211016, 0.1.dev20211017, 0.1.dev20211018, 0.1.dev20211019, 0.1.dev20211020, 0.1.dev20211022, 0.1.dev20211023, 0.1.dev20211024, 0.1.dev20211025, 0.1.dev20211026, 0.1.dev20211027, 0.1.dev20211028, 0.1.dev20211029, 0.1.dev20211030, 0.1.dev20211031, 0.1.dev20211101, 0.1.dev20211102, 0.1.dev20211103, 0.1.dev20211104, 0.1.dev20211105, 0.1.dev20211106, 0.1.dev20211107, 0.1.dev20211108, 0.1.dev20211109, 0.1.dev20211110, 0.1.dev20211111, 0.1.dev20211112, 0.1.dev20211113, 0.1.dev20211114, 0.1.dev20211115, 0.1.dev20211117, 0.1.dev20211118, 0.1.dev20211119, 0.1.dev20211120, 0.1.dev20211121, 0.1.dev20211122, 0.1.dev20211123, 0.1.dev20211124, 0.1.dev20211125, 0.1.dev20211126, 0.1.dev20211127, 0.1.dev20211128, 0.1.dev20211129, 0.1.dev20211130, 0.1.dev20211201, 0.1.dev20211202, 0.1.dev20211203, 0.1.dev20211204, 0.1.dev20211205, 0.1.dev20211206, 0.1.dev20211207, 0.1.dev20211208, 0.1.dev20211209, 0.1.dev20211217, 0.1.dev20211229, 0.1.dev20211230, 0.1.dev20211231, 0.1.dev20220101, 0.1.dev20220102, 0.1.dev20220103, 0.1.dev20220104, 0.1.dev20220105, 0.1.dev20220106, 0.1.dev20220107, 0.1.dev20220108, 0.1.dev20220109, 0.1.dev20220110, 0.1.dev20220111, 0.1.dev20220114, 0.1.dev20220115, 0.1.dev20220116, 0.1.dev20220117, 0.1.dev20220118, 0.1.dev20220119, 0.1.dev20220126, 0.1.dev20220127, 0.1.dev20220128, 0.1.dev20220202, 0.1.dev20220203, 0.1.dev20220204, 0.1.dev20220205, 0.1.dev20220206, 0.1.dev20220207, 0.1.dev20220208, 0.1.dev20220209, 0.1.dev20220210, 0.1.dev20220211, 0.1.dev20220212, 0.1.dev20220213, 0.1.dev20220214, 0.1.dev20220215, 0.1.dev20220216, 0.1.dev20220217, 0.1.dev20220218, 0.1.dev20220219, 0.1.dev20220220, 0.1.dev20220221, 0.1.dev20220222, 0.1.dev20220223, 0.1.dev20220224, 0.1.dev20220225, 0.1.dev20220226, 0.1.dev20220227, 0.1.dev20220228, 0.1.dev20220301, 0.1.dev20220303, 0.1.dev20220304, 0.1.dev20220305, 0.1.dev20220306, 0.1.dev20220307, 0.1.dev20220308, 0.1.dev20220309, 0.1.dev20220310, 0.1.dev20220311, 0.1.dev20220312, 0.1.dev20220313, 0.1.dev20220314, 0.1.dev20220315, 0.1.dev20220316, 0.1.dev20220317, 0.1.dev20220318, 0.1.dev20220319, 0.1.dev20220320, 0.1.dev20220321, 0.1.dev20220322, 0.1.dev20220323, 0.1.dev20220324, 0.1.dev20220325, 0.1.dev20220326, 0.1.dev20220327, 0.1.dev20220328, 0.1.dev20220329, 0.1.dev20220407, 0.1.dev20220408, 0.1.dev20220409, 0.1.dev20220410, 0.1.dev20220411, 0.1.dev20220412, 0.1.dev20220413, 0.1.dev20220414, 0.1.dev20220415, 0.1.dev20220416, 0.1.dev20220417, 0.1.dev20220418, 0.1.dev20220419, 0.1.dev20220420, 0.1.dev20220503, 0.1.dev20220504, 0.1.dev20220505, 0.1.dev20220506, 0.1.dev20220507, 0.1.dev20220509, 0.1.dev20220510, 0.1.dev20220511, 0.1.dev20220515, 0.1.dev20220516, 0.1.dev20220517, 0.1.dev20220518, 0.1.dev20220520, 0.1.dev20220521, 0.1.dev20220522, 0.1.dev20220524, 0.1.dev20220526, 0.1.dev20220527, 0.1.dev20220528, 0.1.dev20220529, 0.1.dev20220530, 0.1.dev20220531, 0.1.dev20220601, 0.1.dev20220602, 0.1.dev20220603, 0.1.dev20220604, 0.1.dev20220605, 0.1.dev20220606, 0.1.dev20220607, 0.1.dev20220608, 0.1.dev20220609, 0.1.dev20220610, 0.1.dev20220611, 0.1.dev20220612, 0.1.dev20220613, 0.1.dev20220614, 0.1.dev20220615, 0.1.dev20220616, 0.1.dev20220617, 0.1.dev20220618, 0.1.dev20220619, 0.1.dev20220620, 0.1.dev20220621, 0.1.dev20220622, 0.1.dev20220623, 0.1.dev20220624, 0.1.dev20220625, 0.1.dev20220626, 0.1.dev20220627, 0.1.dev20220628, 0.1.dev20220629, 0.1.dev20220630, 0.1.dev20220701, 0.1.dev20220702, 0.1.dev20220703, 0.1.dev20220704, 0.1.dev20220705, 0.1.dev20220706, 0.1.dev20220707, 0.1.dev20220708, 0.1.dev20220709, 0.1.dev20220710, 0.1.dev20220711, 0.1.dev20220713, 0.1.dev20220714, 0.1.dev20220715, 0.1.dev20220716, 0.1.dev20220717, 0.1.dev20220718, 0.1.dev20220719, 0.1.dev20220720, 0.1.dev20220721, 0.1.dev20220722, 0.1.dev20220723, 0.1.dev20220724, 0.1.dev20220725, 0.1.dev20220726, 0.1.dev20220727, 0.1.dev20220728, 0.1.dev20220729, 0.1.dev20220730, 0.1.dev20220731, 0.1.dev20220801, 0.1.dev20220802, 0.1.dev20220803, 0.1.dev20220804, 0.1.dev20220806, 0.1.dev20220807, 0.1.dev20220808, 0.1.dev20220809, 0.1.dev20220810, 0.1.dev20220812, 0.1.dev20220813, 0.1.dev20220814, 0.1.dev20220815, 0.1.dev20220816, 0.1.dev20220824, 0.1.dev20220908, 0.1.dev20220909, 0.1.dev20220910, 0.1.dev20220912, 0.1.dev20220913, 0.1.dev20220914, 0.1.dev20220915, 0.1.dev20220916, 0.1.dev20220917, 0.1.dev20220918, 0.1.dev20220919, 0.1.dev20220920, 0.1.dev20220921, 0.1.dev20220922, 0.1.dev20220923, 0.1.dev20220924, 0.1.dev20220925, 0.1.dev20220926, 0.1.dev20220927, 0.1.dev20220928, 0.1.dev20220929, 0.1.dev20220930, 0.1.dev20221001, 0.1.dev20221002, 0.1.dev20221003, 0.1.dev20221004, 0.1.dev20221005, 0.1.dev20221006, 0.1.dev20221007, 0.1.dev20221008, 0.1.dev20221009, 0.1.dev20221010, 0.1.dev20221011, 0.1.dev20221012, 0.1.dev20221013, 0.1.dev20221014, 0.1.dev20221015, 0.1.dev20221016, 0.1.dev20221017, 0.1.dev20221018, 0.1.dev20221019, 0.1.dev20221020, 0.1.dev20221021, 0.1.dev20221022, 0.1.dev20221023, 0.1.dev20221024, 0.1.dev20221025, 0.1.dev20221026, 0.1.dev20221027, 0.1.dev20221028, 0.1.dev20221029, 0.1.dev20221030, 0.1.dev20221031, 0.1.dev20221101, 0.1.dev20221102, 0.1.dev20221103, 0.1.dev20221104, 0.1.dev20221105, 0.1.dev20221106, 0.1.dev20221107, 0.1.dev20221108, 0.1.dev20221109, 0.1.dev20221115, 0.1.dev20221117, 0.1.dev20221118, 0.1.dev20221119, 0.1.dev20221120, 0.1.dev20221121, 0.1.dev20221122, 0.1.dev20221123, 0.1.dev20221124, 0.1.dev20221125, 0.1.dev20221126, 0.1.dev20221127, 0.1.dev20221128, 0.1.dev20221129, 0.1.dev20221130, 0.1.dev20221201, 0.1.dev20221202, 0.1.dev20221203, 0.1.dev20221204, 0.1.dev20221205, 0.1.dev20221206, 0.1.dev20221207, 0.1.dev20221208, 0.1.dev20221209, 0.1.dev20221210, 0.1.dev20221211, 0.1.dev20221212, 0.1.dev20221213, 0.1.dev20221214, 0.1.dev20221216, 0.1.dev20221217, 0.1.dev20221218, 0.1.dev20221219, 0.1.dev20221220, 0.1.dev20221221, 0.1.dev20221222, 0.1.dev20221223, 0.1.dev20221224, 0.1.dev20221225, 0.1.dev20221226, 0.1.dev20221227, 0.1.dev20221228, 0.1.dev20221229, 0.1.dev20221230, 0.1.dev20221231, 0.1.dev20230101, 0.1.dev20230102, 0.1.dev20230103, 0.1.dev20230104, 0.1.dev20230105, 0.1.dev20230106, 0.1.dev20230107, 0.1.dev20230108, 0.1.dev20230109, 0.1.dev20230110, 0.1.dev20230111, 0.1.dev20230112, 0.1.dev20230113, 0.1.dev20230114, 0.1.dev20230115, 0.1.dev20230116, 0.1.dev20230117, 0.1.dev20230118, 0.1.dev20230119, 0.1.dev20230120, 0.1.dev20230121, 0.1.dev20230122, 0.1.dev20230123, 0.1.dev20230124, 0.1.dev20230125, 0.1.dev20230126, 0.1.dev20230127, 0.1.dev20230128, 0.1.dev20230129, 0.1.dev20230130, 0.1.dev20230131, 0.1.dev20230201, 0.1.dev20230202, 0.1.dev20230203, 0.1.dev20230204, 0.1.dev20230205, 0.1.dev20230206, 0.1.dev20230207, 0.1.dev20230208, 0.1.dev20230209, 0.1.dev20230210, 0.1.dev20230211, 0.1.dev20230212, 0.1.dev20230213, 0.1.dev20230214, 0.1.dev20230215, 0.1.dev20230216, 0.1.dev20230217, 0.1.dev20230218, 0.1.dev20230219, 0.1.dev20230220, 0.1.dev20230221, 0.1.dev20230222, 0.1.dev20230223, 0.1.dev20230224, 0.1.dev20230225, 0.1.dev20230226, 0.1.dev20230228, 0.1.dev20230301, 0.1.dev20230302, 0.1.dev20230303, 0.1.dev20230304, 0.1.dev20230305, 0.1.dev20230306, 0.1.dev20230307, 0.1.dev20230308, 0.1.dev20230309, 0.1.dev20230310, 0.1.dev20230311, 0.1.dev20230312, 0.1.dev20230313, 0.1.dev20230314, 0.1.dev20230315, 0.1.dev20230316, 0.1.dev20230317, 0.1.dev20230318, 0.1.dev20230319, 0.1.dev20230320, 0.1.dev20230321, 0.1.dev20230322, 0.1.dev20230323, 0.1.dev20230324, 0.1.dev20230325, 0.1.dev20230326, 0.1.dev20230327, 0.1.dev20230328, 0.1.dev20230329, 0.1.dev20230330, 0.1.dev20230331, 0.1.dev20230401, 0.1.dev20230402, 0.1.dev20230403, 0.1.dev20230404, 0.1.dev20230405, 0.1.dev20230406, 0.1.dev20230407, 0.1.dev20230408, 0.1.dev20230409, 0.1.dev20230410, 0.1.dev20230411, 0.1.dev20230412, 0.1.dev20230413, 0.1.dev20230414, 0.1.dev20230415, 0.1.dev20230416, 0.1.dev20230417, 0.1.dev20230418, 0.1.dev20230419, 0.1.dev20230420, 0.1.dev20230421, 0.1.dev20230422, 0.1.dev20230423, 0.1.dev20230424, 0.1.dev20230425, 0.1.dev20230426, 0.1.dev20230427, 0.1.dev20230428, 0.1.dev20230429, 0.1.dev20230430, 0.1.dev20230501, 0.1.dev20230502, 0.1.dev20230503, 0.1.dev20230504, 0.1.dev20230505, 0.1.dev20230506, 0.1.dev20230507, 0.1.dev20230508, 0.1.dev20230509, 0.1.dev20230510, 0.1.dev20230511, 0.1.dev20230512, 0.1.dev20230513, 0.1.dev20230514, 0.1.dev20230515, 0.1.dev20230516, 0.1.dev20230517, 0.1.dev20230518, 0.1.dev20230519, 0.1.dev20230520, 0.1.dev20230521, 0.1.dev20230522, 0.1.dev20230523, 0.1.dev20230524, 0.1.dev20230525, 0.1.dev20230526, 0.1.dev20230527, 0.1.dev20230528, 0.1.dev20230529, 0.1.dev20230530, 0.1.dev20230531, 0.1.dev20230601, 0.1.dev20230602, 0.1.dev20230603, 0.1.dev20230604, 0.1.dev20230605, 0.1.dev20230606, 0.1.dev20230607, 0.1.dev20230608, 0.1.dev20230609, 0.1.dev20230610, 0.1.dev20230611, 0.1.dev20230612, 0.1.dev20230613, 0.1.dev20230614, 0.1.dev20230615, 0.1.dev20230616, 0.1.dev20230617, 0.1.dev20230618, 0.1.dev20230619, 0.1.dev20230620, 0.1.dev20230621, 0.1.dev20230622, 0.1.dev20230623, 0.1.dev20230624, 0.1.dev20230625, 0.1.dev20230626, 0.1.dev20230627, 0.1.dev20230628, 0.1.dev20230629+default, 0.1.dev20230630+default, 0.1.dev20230701+default, 0.1.dev20230702+default, 0.1.dev20230703+default, 0.1.dev20230704+default, 0.1.dev20230705+default, 0.1.dev20230706+default, 0.1.dev20230707+default, 0.1.dev20230708+default, 0.1.dev20230709+default, 0.1.dev20230710+default, 0.1.dev20230711+default, 0.1.dev20230712+default, 0.1.dev20230715+default, 0.1.dev20230716+default, 0.1.dev20230717+default, 0.1.dev20230718+default, 0.1.dev20230719+default, 0.1.dev20230720+default, 0.1.dev20230721+default, 0.1.dev20230722+default, 0.1.dev20230723+default, 0.1.dev20230724+default, 0.1.dev20230725+default, 0.1.dev20230726+default, 0.1.dev20230727+default, 0.1.dev20230802+default, 0.1.dev20230803+default, 0.1.dev20230804+default, 0.1.dev20230805+default, 0.1.dev20230806+default, 0.1.dev20230807+default, 0.1.dev20230808+default, 0.1.dev20230809+default, 0.1.dev20230810+default, 0.1.dev20230815+default, 0.1.dev20230816+default, 0.1.dev20230817+default, 0.1.dev20230818+default, 0.1.dev20230819+default, 0.1.dev20230820+default, 0.1.dev20230821+default, 0.1.dev20230823+default, 0.1.dev20230824+default, 0.1.dev20230825+default, 0.1.dev20230826+default, 0.1.dev20230827+default, 0.1.dev20230828+default, 0.1.dev20230829+default, 0.1.dev20230830+default, 0.1.dev20230831+default, 0.1.dev20230911+default, 0.1.dev20230912+default, 0.1.dev20230913+default, 0.1.dev20230914+default, 0.1.dev20230915+default, 0.1.dev20230916+default, 0.1.dev20230917+default, 0.1.dev20230918+default, 0.1.dev20230919+default, 0.1.dev20230920+default, 0.1.dev20230921+default, 0.1.dev20230922+default, 0.1.dev20230923+default, 0.1.dev20230924+default, 0.1.dev20230925+default, 0.1.dev20230926+default, 0.1.dev20230927+default, 0.1.dev20230928+default, 0.1.dev20230929+default, 0.1.dev20230930+default, 0.1.dev20231001+default, 0.1.dev20231002+default, 0.1.dev20231003+default, 0.1.dev20231004+default, 0.1.dev20231005+default, 0.1.dev20231006+default, 0.1.dev20231007+default, 0.1.dev20231008+default, 0.1.dev20231009+default, 0.1.dev20231010+default, 0.1.dev20231011+default, 0.1.dev20231012+default, 0.1.dev20231013+default, 0.1.dev20231014+default, 0.1.dev20231015+default, 0.1.dev20231016+default, 0.1.dev20231017+default, 0.1.dev20231018+default, 0.1.dev20231019+default, 0.1.dev20231020+default, 0.1.dev20231021+default, 0.1.dev20231022+default, 0.1.dev20231023+default, 0.1.dev20231024+default, 0.1.dev20231025+default, 0.1.dev20231026+default, 0.1.dev20231027+default, 0.1.dev20231028+default, 0.1.dev20231029+default, 0.1.dev20231030+default, 0.1.dev20231031+default, 0.1.dev20231101+default, 0.1.dev20231102+default, 0.1.dev20231103+default, 0.1.dev20231104+default, 0.1.dev20231105+default, 0.1.dev20231106+default, 0.1.dev20231107+default, 0.1.dev20231114+default, 0.1.dev20231115+default, 0.1.dev20231116+default, 0.1.dev20231117+default, 0.1.dev20231118+default, 0.1.dev20231119+default, 0.1.dev20231120+default, 0.1.dev20231121+default, 0.1.dev20231122+default, 0.1.dev20231123+default, 0.1.dev20231124+default, 0.1.dev20231125+default, 0.1.dev20231126+default, 0.1.dev20231127+default, 0.1.dev20231128+default, 0.1.dev20231129+default, 0.1.dev20231130+default, 0.1.dev20231201+default, 0.1.dev20231202+default, 0.1.dev20231203+default, 0.1.dev20231204+default, 0.1.dev20231205+default, 0.1.dev20231206+default, 0.1.dev20231207+default, 0.1.dev20231208+default, 0.1.dev20231209+default, 0.1.dev20231210+default, 0.1.dev20231211+default, 0.1.dev20231212+default, 0.1.dev20231213+default, 0.1.dev20231214+default, 0.1.dev20231215+default, 0.1.dev20231216+default, 0.1.dev20231217+default, 0.1.dev20231218+default, 0.1.dev20231219+default, 0.1.dev20231220+default, 0.1.dev20231221+default, 0.1.dev20231222+default, 0.1.dev20231223+default, 0.1.dev20231224+default, 0.1.dev20231225+default, 0.1.dev20231226+default, 0.1.dev20231227+default, 0.1.dev20231228+default, 0.1.dev20231229+default, 0.1.dev20231230+default, 0.1.dev20231231+default, 0.1.dev20240101+default, 0.1.dev20240102+default, 0.1.dev20240103+default, 0.1.dev20240104+default, 0.1.dev20240105+default, 0.1.dev20240106+default, 0.1.dev20240107+default, 0.1.dev20240108+default, 0.1.dev20240109+default, 0.1.dev20240110+default, 0.1.dev20240111+default, 0.1.dev20240113+default, 0.1.dev20240114+default, 0.1.dev20240115+default, 0.1.dev20240116+default, 0.1.dev20240117+default, 0.1.dev20240118+default, 0.1.dev20240119+default, 0.1.dev20240120+default, 0.1.dev20240121+default, 0.1.dev20240122+default, 0.1.dev20240123+default, 0.1.dev20240124+default, 0.1.dev20240127+default, 0.1.dev20240128+default, 0.1.dev20240129+default, 0.1.dev20240130+default, 0.1.dev20240131+default, 0.1.dev20240201+default, 0.1.dev20240202+default, 0.1.dev20240203+default, 0.1.dev20240204+default, 0.1.dev20240205+default, 0.1.dev20240206+default, 0.1.dev20240207+default, 0.1.dev20240208+default, 0.1.dev20240209+default, 0.1.dev20240210+default, 0.1.dev20240213+default, 0.1.dev20240214+default, 0.1.dev20240215+default, 0.1.dev20240216+default, 0.1.dev20240217+default, 0.1.dev20240218+default, 0.1.dev20240219+default, 0.1.dev20240220+default, 0.1.dev20240221+default, 0.1.dev20240222+default, 0.1.dev20240223+default, 0.1.dev20240224+default, 0.1.dev20240225+default, 0.1.dev20240226+default, 0.1.dev20240227+default, 0.1.dev20240228+default, 0.1.dev20240229+default, 0.1.dev20240301+default, 0.1.dev20240302+default, 0.1.dev20240303+default, 0.1.dev20240304+default, 0.1.dev20240305+default, 0.1.dev20240306+default, 0.1.dev20240307+default, 0.1.dev20240308+default, 0.1.dev20240309+default, 0.1.dev20240310+default, 0.1.dev20240311+default, 0.1.dev20240312+default, 0.1.dev20240313+default, 0.1.dev20240314+default, 0.1.dev20240315+default, 0.1.dev20240316+default, 0.1.dev20240317+default, 0.1.dev20240318+default, 0.1.dev20240319+default, 0.1.dev20240320+default, 0.1.dev20240321+default, 0.1.dev20240322+default, 0.1.dev20240323+default, 0.1.dev20240324+default, 0.1.dev20240325+default, 0.1.dev20240326+default, 0.1.dev20240327+default, 0.1.dev20240328+default, 0.1.dev20240329+default, 0.1.dev20240330+default, 0.1.dev20240331+default, 0.1.dev20240401+default, 0.1.dev20240402+default, 0.1.dev20240403+default, 0.1.dev20240404+default, 0.1.dev20240405+default, 0.1.dev20240406+default, 0.1.dev20240407+default, 0.1.dev20240408+default, 0.1.dev20240409+default, 0.1.dev20240410+default, 0.1.dev20240411+default, 0.1.dev20240412+default, 0.1.dev20240413+default, 0.1.dev20240414+default, 0.1.dev20240415+default, 0.1.dev20240416+default, 0.1.dev20240417+default, 0.1.dev20240418+default, 0.1.dev20240419+default, 0.1.dev20240420+default, 0.1.dev20240421+default, 0.1.dev20240422+default, 0.1.dev20240423+default, 0.1.dev20240424+default, 0.1.dev20240425+default, 0.1.dev20240426+default, 0.1.dev20240427+default, 0.1.dev20240428+default, 0.1.dev20240429+default, 0.1.dev20240430+default, 0.1.dev20240501+default, 0.1.dev20240502+default, 0.1.dev20240503+default, 0.1.dev20240504+default, 0.1.dev20240505+default, 0.1.dev20240506+default, 0.1.dev20240507+default, 0.1.dev20240508+default, 0.1.dev20240509+default, 0.1.dev20240510+default, 0.1.dev20240511+default, 0.1.dev20240512+default, 0.1.dev20240513+default, 0.1.dev20240515+default, 0.1.dev20240516+default, 0.1.dev20240517+default, 0.1.dev20240518+default, 0.1.dev20240519+default, 0.1.dev20240520+default, 0.1.dev20240521+default, 0.1.dev20240522+default, 0.1.dev20240523+default, 0.1.dev20240524+default, 0.1.dev20240525+default, 0.1.dev20240526+default, 0.1.dev20240527+default, 0.1.dev20240528+default, 0.1.dev20240529+default, 0.1.dev20240531+default, 0.1.dev20240601+default, 0.1.dev20240602+default, 0.1.dev20240603+default, 0.1.dev20240604+default, 0.1.dev20240605+default, 0.1.dev20240606+default, 0.1.dev20240607+default, 0.1.dev20240608+default, 0.1.dev20240609+default, 0.1.dev20240610+default, 0.1.dev20240611+default, 0.1.dev20240612+default, 0.1.dev20240613+default, 0.1.dev20240614+default, 0.1.dev20240615+default, 0.1.dev20240616+default, 0.1.dev20240617+default, 0.1.dev20240618+default, 0.1.dev20240619+default, 0.1.dev20240620+default, 0.1.dev20240621+default, 0.1.dev20240622+default, 0.1.dev20240623+default, 0.1.dev20240624+default, 0.1.dev20240625+default, 0.1.dev20240626+default, 0.1.dev20240627+default, 0.1.dev20240628+default, 0.1.dev20240629+default, 0.1.dev20240630+default, 0.1.dev20240701+default, 0.1.dev20240702+default, 0.1.dev20240703+default, 0.1.dev20240704+default, 0.1.dev20240705+default, 0.1.dev20240706+default, 0.1.dev20240707+default, 0.1.dev20240708+default, 0.1.dev20240709+default, 0.1.dev20240710+default, 0.1.dev20240711+default, 0.1.dev20240712+default, 0.1.dev20240713+default, 0.1.dev20240714+default, 0.1.dev20240715+default, 0.1.dev20240716+default, 0.1.dev20240717+default, 0.1.dev20240718+default, 0.1.dev20240719+default, 0.1.dev20240720+default, 0.1.dev20240721+default, 0.1.dev20240722+default, 0.1.dev20240723+default, 0.1.dev20240724+default, 0.1.dev20240725+default, 0.1.dev20240726+default, 0.1.dev20240727+default, 0.1.dev20240728+default, 0.1.dev20240729+default, 0.1.dev20240730+default, 0.1.dev20240731+nightly, 0.1.dev20240801+nightly, 0.1.dev20240802+nightly, 0.1.dev20240803+nightly, 0.1.dev20240804+nightly, 0.1.dev20240805+nightly, 0.1.dev20240806+nightly, 0.1.dev20240807+nightly, 0.1.dev20240808+nightly, 0.1.dev20240809+nightly, 0.1.dev20240810+nightly, 0.1.dev20240811+nightly, 0.1.dev20240812+nightly, 0.1.dev20240813+nightly, 0.1.dev20240814+nightly, 0.1.dev20240815+nightly, 0.1.dev20240816+nightly, 0.1.dev20240817+nightly, 0.1.dev20240818+nightly, 0.1.dev20240819+nightly, 0.1.dev20240821+nightly, 0.1.dev20240822+nightly, 0.1.dev20240823+nightly, 0.1.dev20240824+nightly, 0.1.dev20240825+nightly, 0.1.dev20240826+nightly, 0.1.dev20240827+nightly, 0.1.dev20240828+nightly, 0.1.dev20240829+nightly, 0.1.dev20240830+nightly, 0.1.dev20240831+nightly, 0.1.dev20240901+nightly, 0.1.dev20240902+nightly, 0.1.dev20240903+nightly, 0.1.dev20240905+nightly, 0.1.dev20240906+nightly, 0.1.dev20240907+nightly, 0.1.dev20240908+nightly, 0.1.dev20240909+nightly, 0.1.dev20240910+nightly, 0.1.dev20240911+nightly, 0.1.dev20240912+nightly, 0.1.dev20240913+nightly, 0.1.dev20240914+nightly, 0.1.dev20240915+nightly, 0.1.dev20240916+nightly, 0.1.dev20240917+nightly, 0.1.dev20240918+nightly, 0.1.dev20240919+nightly, 0.1.dev20240920+nightly, 0.1.dev20240921+nightly, 0.1.dev20240922+nightly, 0.1.dev20240923+nightly, 0.1.dev20240924+nightly, 0.1.dev20240925+nightly, 0.1.dev20240926+nightly, 0.1.dev20240927+nightly, 0.1.dev20240928+nightly, 0.1.dev20240929+nightly, 0.1.dev20240930+nightly, 0.1.dev20241001+nightly, 0.1.dev20241002+nightly, 0.1.dev20241003+nightly, 0.1.dev20241004+nightly, 0.1.dev20241005+nightly, 0.1.dev20241006+nightly, 0.1.dev20241007+nightly, 0.1.dev20241008+nightly, 0.1.dev20241009+nightly, 0.1.dev20241010+nightly, 0.1.dev20241010+nightly.cleanup, 0.1.dev20241011+nightly, 0.1.dev20241012+nightly, 0.1.dev20241013+nightly, 0.1.dev20241014+nightly, 0.1.dev20241015+nightly, 0.1.dev20241016+nightly, 0.1.dev20241017+nightly, 0.1.dev20241018+nightly, 0.1.dev20241019+nightly, 0.1.dev20241020+nightly, 0.1.dev20241021+nightly, 0.1.dev20241022+nightly, 0.1.dev20241023+nightly, 0.1.dev20241024+nightly, 0.1.dev20241025+nightly, 0.1.dev20241026+nightly, 0.1.dev20241027+nightly, 0.1.dev20241028+nightly, 0.1.dev20241029+nightly, 0.1.dev20241030+nightly, 0.1.dev20241031+nightly, 0.1.dev20241101+nightly, 0.1.dev20241102+nightly, 0.1.dev20241103+nightly, 0.1.dev20241104+nightly, 0.1.dev20241105+nightly, 0.1.dev20241106+nightly, 0.1.dev20241107+nightly, 0.1.dev20241108+nightly, 0.1.dev20241109+nightly, 0.1.dev20241110+nightly, 0.1.dev20241111+nightly, 0.1.dev20241112+nightly, 0.1.dev20241113+nightly, 0.1.dev20241114+nightly, 0.1.dev20241115+nightly, 0.1.dev20241116+nightly, 0.1.dev20241117+nightly, 0.1.dev20241118+nightly, 0.1.dev20241119+nightly, 0.1.dev20241120+nightly, 0.1.dev20241121+nightly, 0.1.dev20241122+nightly, 0.1.dev20241123+nightly, 0.1.dev20241124+nightly, 0.1.dev20241125+nightly, 0.1.dev20241126+nightly, 0.1.dev20241127+nightly, 0.1.dev20241128+nightly, 0.1.dev20241129+nightly, 0.1.dev20241130+nightly, 0.1.dev20241201+nightly, 0.1.dev20241202+nightly, 0.1.dev20241203+nightly, 0.1.dev20241204+nightly, 0.1.dev20241205+nightly, 0.1.dev20241206+nightly, 0.1.dev20241207+nightly, 0.1.dev20241208+nightly, 0.1.dev20241209+nightly, 0.1.dev20241210+nightly, 0.1.dev20241211+nightly, 0.1.dev20241212+nightly, 0.1.dev20241213+nightly, 0.1.dev20241214+nightly, 0.1.dev20241215+nightly, 0.1.dev20241216+nightly, 0.1.dev20241217+nightly, 0.1.dev20241218+nightly, 0.1.dev20241219+nightly, 0.1.dev20241220+nightly, 0.1.dev20241221+nightly, 0.1.dev20241222+nightly, 0.1.dev20241223+nightly, 0.1.dev20241224+nightly, 0.1.dev20241225+nightly, 0.1.dev20241226+nightly, 0.1.dev20241227+nightly, 0.1.dev20241228+nightly, 0.1.dev20241229+nightly, 0.1.dev20241230+nightly, 0.1.dev20241231+nightly, 0.1.dev20250101+nightly, 0.1.dev20250102+nightly, 0.1.dev20250103+nightly, 0.1.dev20250104+nightly, 0.1.dev20250105+nightly, 0.1.dev20250106+nightly, 0.1.dev20250107+nightly, 0.1.dev20250108+nightly, 0.1.dev20250109+nightly, 0.1.dev20250110+nightly, 0.1.dev20250111+nightly, 0.1.dev20250112+nightly, 0.1.dev20250113+nightly, 0.1.dev20250114+nightly, 0.1.dev20250115+nightly, 0.1.dev20250116+nightly, 0.1.dev20250117+nightly, 0.1.dev20250118+nightly, 0.1.dev20250119+nightly, 0.1.dev20250120+nightly, 0.1.dev20250121+nightly, 0.1.dev20250122+nightly, 0.1.dev20250123+nightly, 0.1.dev20250124+nightly, 0.1.dev20250128+nightly, 0.1.dev20250129+nightly, 0.1.dev20250130+nightly, 0.1.dev20250131+nightly, 0.1.dev20250201+nightly, 0.1.dev20250202+nightly, 0.1.dev20250203+nightly, 0.1.dev20250204+nightly, 0.1.dev20250205+nightly, 0.1.dev20250206+nightly, 0.1.dev20250207+nightly, 0.1.dev20250208+nightly, 0.1.dev20250209+nightly, 0.1.dev20250210+nightly, 0.1.dev20250211+nightly, 0.1.dev20250213+nightly, 0.1.dev20250214+nightly, 0.1.dev20250215+nightly, 0.1.dev20250216+nightly, 0.1.dev20250217+nightly, 0.1.dev20250218+nightly, 0.1.dev20250219+nightly, 0.1.dev20250220+nightly, 0.1.dev20250221+nightly, 0.1.dev20250222+nightly, 0.1.dev20250223+nightly, 0.1.dev20250224+nightly, 0.1.dev20250225+nightly, 0.1.dev20250228+nightly, 0.1.dev20250301+nightly, 0.1.dev20250302+nightly, 0.1.dev20250303+nightly, 0.1.dev20250304+nightly, 0.1.dev20250305+nightly, 0.1.dev20250306+nightly, 0.1.dev20250307+nightly, 0.1.dev20250308+nightly, 0.1.dev20250309+nightly, 0.1.dev20250310+nightly, 0.1.dev20250311+nightly, 0.1.dev20250312+nightly, 0.1.dev20250313+nightly, 0.1.dev20250314+nightly, 0.1.dev20250315+nightly, 0.1.dev20250316+nightly, 0.1.dev20250317+nightly, 0.1.dev20250318+nightly, 0.1.dev20250319+nightly, 0.1.dev20250320+nightly, 0.1.dev20250321+nightly, 0.1.dev20250322+nightly, 0.1.dev20250323+nightly, 0.1.dev20250324+nightly, 0.1.dev20250325+nightly, 0.1.dev20250326+nightly, 0.1.dev20250327+nightly, 0.1.dev20250328+nightly, 0.1.dev20250329+nightly, 0.1.dev20250330+nightly, 0.1.dev20250331+nightly, 0.1.dev20250401+nightly, 0.1.dev20250402+nightly, 0.1.dev20250403+nightly, 0.1.dev20250404+nightly, 0.1.dev20250405+nightly, 0.1.dev20250406+nightly, 0.1.dev20250407+nightly, 0.1.dev20250415+nightly, 0.1.dev20250417+nightly, 0.1.dev20250418+nightly, 0.1.dev20250419+nightly, 0.1.dev20250420+nightly, 0.1.dev20250421+nightly, 0.1.dev20250423+nightly, 0.1.dev20250424+nightly, 0.1.dev20250425+nightly, 0.1.dev20250426+nightly, 0.1.dev20250427+nightly, 0.1.dev20250428+nightly, 0.1.dev20250429+nightly, 0.1.dev20250430+nightly, 0.1.dev20250501+nightly, 0.1.dev20250502+nightly, 0.1.dev20250503+nightly, 0.1.dev20250504+nightly, 0.1.dev20250505+nightly, 0.1.dev20250513+nightly, 0.1.dev20250514+nightly, 0.1.dev20250515+nightly, 0.1.dev20250516+nightly, 0.1.dev20250517+nightly, 0.1.dev20250518+nightly, 0.1.dev20250519+nightly, 0.1.dev20250520+nightly, 0.1.dev20250521+nightly, 0.1.dev20250522+nightly, 0.1.dev20250523+nightly, 0.1.dev20250524+nightly, 0.1.dev20250525+nightly, 0.1.dev20250526+nightly, 0.1.dev20250527+nightly, 0.1.dev20250528+nightly, 0.1.dev20250529+nightly, 0.1.dev20250530+nightly, 0.1.dev20250531+nightly, 0.1.dev20250601+nightly, 0.1.dev20250603+nightly, 0.1.dev20250604+nightly, 0.1.dev20250605+nightly, 0.1.dev20250606+nightly, 0.1.dev20250607+nightly, 0.1.dev20250608+nightly, 0.1.dev20250609+nightly, 0.1.dev20250610+nightly, 0.1.dev20250611+nightly, 0.1.dev20250612+nightly, 0.1.dev20250613+nightly, 0.1.dev20250614+nightly, 0.1.dev20250615+nightly, 0.1.dev20250616+nightly, 0.1.dev20250617+nightly, 0.1.dev20250618+nightly, 0.1.dev20250619+nightly, 0.1.dev20250620+nightly, 0.1.dev20250621+nightly, 0.1.dev20250622+nightly, 0.1.dev20250623+nightly, 0.1.dev20250624+nightly, 0.1.dev20250625+nightly, 0.1.dev20250626+nightly, 0.1.dev20250627+nightly, 0.1.dev20250628+nightly, 0.1.dev20250629+nightly, 0.1.dev20250630+nightly, 0.1.dev20250701+nightly, 0.1.dev20250702+nightly, 0.1.dev20250703+nightly, 0.1.dev20250704+nightly, 0.1.dev20250705+nightly, 0.1.dev20250706+nightly, 0.1.dev20250707+nightly, 0.1.dev20250708+nightly, 0.1.dev20250709+nightly, 0.1.dev20250710+nightly, 0.1.dev20250711+nightly, 0.1.dev20250712+nightly, 0.1.dev20250713+nightly, 0.1.dev20250714+nightly, 0.1.dev20250715+nightly, 0.1.dev20250716+nightly, 0.1.dev20250717+nightly, 0.1.dev20250718+nightly, 0.1.dev20250719+nightly, 0.1.dev20250720+nightly, 0.1.dev20250721+nightly, 0.1.dev20250722+nightly, 0.1.dev20250723+nightly, 0.1.dev20250724+nightly, 0.1.dev20250725+nightly, 0.1.dev20250726+nightly, 0.1.dev20250727+nightly, 0.1.dev20250728+nightly, 0.1.dev20250729+nightly, 0.1.dev20250730+nightly, 0.1.dev20250731+nightly, 0.1.dev20250802+nightly, 0.1.dev20250803+nightly, 0.1.dev20250804+nightly, 0.1.dev20250807+nightly, 0.1.dev20250808+nightly, 0.1.dev20250809+nightly, 0.1.dev20250810+nightly, 0.1.dev20250811+nightly, 0.1.dev20250812+nightly, 0.1.dev20250813+nightly, 0.1.dev20250814+nightly)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for libtpu-nightly\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: jax[tpu] in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Collecting jax[tpu]\n",
            "  Using cached jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.8.0,>=0.8.0 (from jax[tpu])\n",
            "  Using cached jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax[tpu]) (0.5.3)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from jax[tpu]) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax[tpu]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax[tpu]) (1.16.3)\n",
            "Collecting libtpu==0.0.24.* (from jax[tpu])\n",
            "  Downloading libtpu-0.0.24-cp312-cp312-manylinux_2_31_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from jax[tpu]) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]) (2025.10.5)\n",
            "Downloading libtpu-0.0.24-cp312-cp312-manylinux_2_31_x86_64.whl (156.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl (79.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.8.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libtpu, jaxlib, jax\n",
            "  Attempting uninstall: libtpu\n",
            "    Found existing installation: libtpu 0.0.17\n",
            "    Uninstalling libtpu-0.0.17:\n",
            "      Successfully uninstalled libtpu-0.0.17\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.2\n",
            "    Uninstalling jaxlib-0.7.2:\n",
            "      Successfully uninstalled jaxlib-0.7.2\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.2\n",
            "    Uninstalling jax-0.7.2:\n",
            "      Successfully uninstalled jax-0.7.2\n",
            "Successfully installed jax-0.8.0 jaxlib-0.8.0 libtpu-0.0.24\n"
          ]
        }
      ]
    }
  ]
}